# 設計メモ

## 内容

### 概要

* 数字の 0 か 1 かを判定するニューラルネットワーク(以下NNWと記載)

### 入出力

* 共通
   * 形式
      * 文字エンコーディング: UTF8
      * 改行コード: LF
      * タブ区切り

* 入力
   * 学習データの画像データ
      * 1行がひとつのデータに対応する
      * 行の内容
         * 3 × 4 マスで数字の 1 か 0 を示す画像を横一列にならべたもの
         * 各々のマスが取る値は、1 か 0 の2通り
         * 詳細は 入力データ.xls 参照

   * 学習データの正解ラベル
      * 行は、画像データに対応する
      * 行の内容
         * 正解ラベル(1 か 0)

   * テストデータ
      * 学習データの画像データと同じ

* 出力
   * テストデータの予測結果
      * 行はテストデータに対応する
      * 行の内容
         * 1列目: 予測結果(0 or 1)
         * 2列目: 出力層のユニット1の出力
         * 3列目: 出力層のユニット2の出力

### NNWの設計

* 構成
   * NNW構成図.xlsx 参照

* タイプ
   * 全結合
   * 中間層の数: 1

* 入力層
   * ユニット数: 12
   * 活性化関数: 恒等関数

* 中間層(隠れ層)
   * ユニット数: 3
   * 活性化関数: シグモイド関数

* 出力層
   * ユニット数: 2
      * ユニット1の出力: 画像データが数字の 0 であることの確信度
      * ユニット2の出力: 画像データが数字の 1 であることの確信度
   * 活性化関数: シグモイド関数

* 最適化手法
   * 勾配降下法

* コスト関数
   * 誤差二乗和

### プログラム設計

#### 処理概要

* 処理概要
   1. 入力データ読み込み
   1. モデルのインスタンス作成
   1. モデルの学習
      * 学習データを用いたNNWの内部パラメーターの最適化
   1. モデルによる予測
      * 学習したモデルを用いたテストデータに対する予測
   1. 出力データ書き出し

* モデルの学習
   * パラメータの初期化
   * 各データに対して以下を繰り返す
      1. Forward
      1. コスト計算
      1. Backpropagation
   * パラメータ更新
   * トータルコスト計算

* モデルによる予測
   1. Forward

### 処理詳細

* Forward
   * 入力層
      * layer_1_out = layer_1_in
         * 恒等関数のため、そのまま
   * 入力層 → 中間層
      * w_1 * layer_1_out = layer_2_in
   * 中間層
      * layer_2_in * シグモイド関数 = layer_2_out
         * numpyのexpを使用する
   * 中間層 → 出力層
      * w_2 * layer_2_out = layer_3_in
   * 出力層
      * layer_3_in * シグモイド関数 = layer_3_out

* コスト計算
   * 1/2{(t1-a1(3))^2 + (t1-a2(3))^2}
      * t1: 正解ラベルが0のとき 1 。それ以外 0
      * t2: 正解ラベルが1のとき 1 。それ以外 0
   * ((layer_3_out-t)**2).sum()

* Backpropagation
   * 出力層の出力ユニットの誤差
      * error_3_out = layer_3_out - t
   * 出力層の入力ユニットの誤差
      * error_3_in = error_3_out * シグモイド関数(layer_3_in){1 - シグモイド関数(layer_3_in)}
   * 重み行列2の微分
      * error_w_2[データ][2][3]
      * error_w_2[データ][i][j] = error_3_in[i] * layer_2_out[j]
      * 結局↓
      * error_w_2[データ][][] = np.array([error_3_in] for i in range(3)).T * layer_2_out
   * バイアス2の微分
      * error_b_2[データ][2]
      * error_b_2[データ][i]  = error_3_in[i]
   * 中間層の出力ユニットの誤差
      * error_2_out[] = w_2.T * error_3_in[]
   * 中間層入力ユニットの誤差
      * error_2_in = error_2_out * シグモイド関数(layer_2_in){1 - シグモイド関数(layer_2_in)}
   * 重み行列1の微分
      * error_w_1[データ][3][12]
      * error_w_1[データ][i][j] = error_2_in[i] * layer_1_out[j]
      * 結局↓
      * error_w_1[データ][][] = np.array([error_2_in] for i in range(12)).T * layer_1_out
   * バイアス1の微分
      * error_b_1[データ][3]
      * error_b_1[データ][i]  = error_2_in[i]

* パラメータ更新
   * パラメータ(l + 1) = パラメータ(l) + イータ * パラメータの微分
      * l: 繰り返し数

#### データ格納先

* 正解ラベル
   * t[データ番号][ユニット番号]
      * データ番号: 画像データの行に対応する
      * ユニット番号: 出力層のユニット番号に対応する
      * t[][0] = 画像が0を示すとき1、1を示すとき0
      * t[][1] = 画像が1を示すとき1、0を示すとき0

* 各層の入力と出力の格納先
   * ※ 出力は入力に活性化関数を適用したもの

   * 入力層
      * layer_1_in[12]
      * layer_1_out[12]

   * 中間層
      * layer_2_in[3]
      * layer_2_out[3]

   * 出力層
      * layer_3_in[2]
      * layer_3_out[2]

* 内部パラメーター
   * 重み行列1
      * 入力層と中間層の間の重み行列
      * w_1[3][12]

   * 重み行列2
      * 中間層と出力層の間の重み行列
      * w_2[2][3]

   * バイアス1
      * 入力層と中間層の間のバイアス
      * b_1[3]

   * バイアス2
      * 中間層と出力層の間のバイアス
      * b_2[2]

* Backpropagationにおける各ユニットの誤差の格納先

   * 出力層の誤差
      * error_3_out[2]
      * error_3_in[2]

   * 重み行列2の微分
      * error_w_2[i][2][3]

   * バイアス2の微分
      * error_b_2[i][2]

   * 中間層の誤差
      * error_2_out[3]
      * error_2_in[3]

   * 重み行列1の微分
      * error_w_1[i][3][12]

   * バイアス1の微分
      * error_b_1[i][3]

* コスト格納先
   * cost[i]
      * 各画像データで計算した際のコスト
   * tota_cost
      * cost[i]の合計

* 補足
   * i: データ番号に対応

### 展望

* 出力層の活性化関数の変更。
   * ソフトマックス関数にする。

* コスト関数の変更。
   * クロスエントロピーにする。